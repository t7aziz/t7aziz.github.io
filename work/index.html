<head>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="/styles/main.css">
</head>
<body>
<div id="app">
<header>
    <nav>
        <a href="mailto:aziztaha082@gmail.com">Email↗</a>
        <a href="/resume/TahaAziz_resume.pdf" target="_blank">Resume↗</a>
        <a href="https://linkedin.com/in/t7aziz" target="_blank">LinkedIn↗</a>
        <a href="https://github.com/t7aziz" target="_blank">GitHub↗</a>
    </nav>
</header>
<section>
    <br>
    <a href="/" data-route>← Back</a>
    <br><h1>Sentia Solutions</h1>
    <p>I first began working at Sentia as a co-op from January 2023 to August 2023 for a total of 8 months as my final co-op placement of my undergraduate career. Directly afterwards, during the final 8 months of my degree, I continued to work for them as a part-time employee. After graduation, I was welcomed to a permanent full-time position at the company which lasted from June 2024 to December 2024. The following is a major project which I worked on, parts of it beginning as a co-op, and more complex elements being finished during my time as a permanent full-time employee.</p>
    <h3 id="-building-a-centralized-log-management-system-a-deep-dive-into-scalability-automation-and-reliability-"><strong>Building a Centralized Log Management System: A Deep Dive into Scalability, Automation, and Reliability</strong></h3>
<p>At Sentia, I led the design and implementation of a <strong>centralized log management system</strong> designed to aggregate and process logs from a wide variety of customer devices which generated logs in different formats and frequencies, presenting unique challenges for data ingestion. This project was a culmination of my studies in data engineering and software architecture, including my implementation of the <strong>medallion architecture</strong>, as well as the use of <strong>Azure Databricks</strong>, <strong>PySpark</strong>, and <strong>Azure Data Factory</strong>, and transformed how we monitored customer infrastructure.</p>
<hr>
<h3 id="-the-problem-siloed-logs-and-delayed-insights-"><strong>The Problem: Siloed Logs and Delayed Insights</strong></h3>
<p>Our customers operated in distributed cloud environments, generating gigabytes of server logs daily from several device types such as <strong>switches, servers, storage devices, virtual machines (VMs), and other network appliances</strong>. These logs were critical for monitoring system health and detecting security breaches. However, the logs were stored in siloed systems across multiple cloud providers (e.g., AWS, Azure, GCP) and on-premise servers. Retrieving and analyzing this data was a manual, time-consuming process, often taking days. This delay hindered incident response and posed significant risks to our service.</p>
<p>The challenge was clear: <strong>How could we consolidate, process, and analyze logs from 50+ diverse environments in near-real time, while ensuring scalability, reliability, and cost efficiency?</strong></p>
<hr>
<h3 id="-the-solution-a-scalable-automated-log-management-system-"><strong>The Solution: A Scalable, Automated Log Management System</strong></h3>
<h4 id="-1-data-ingestion-bridging-diverse-environments-with-azure-data-factory-"><strong>1. Data Ingestion: Bridging Diverse Environments with Azure Data Factory</strong></h4>
<p>The first step was to build a robust data ingestion pipeline that could collect logs from 50+ customer environments, each with its own format, protocol, and storage system. I chose <strong>Azure Data Factory (ADF)</strong> for this task due to its flexibility and scalability.</p>
<ul>
<li><p><strong>Design Pattern</strong>: I implemented a <strong>publisher-subscriber model</strong>, where each customer environment acted as a publisher, sending logs to a centralized ingestion endpoint. ADF served as the subscriber, orchestrating the ingestion process.</p>
</li>
<li><p><strong>Technical Implementation</strong>: I created custom connectors in ADF to handle diverse log formats (e.g., JSON, CSV, syslog) and protocols (e.g., HTTP, FTP, SFTP). For example, I used ADF’s <strong>REST API connector</strong> to pull logs from customer APIs and the <strong>Blob Storage connector</strong> to fetch logs stored in Azure Blob Storage.</p>
</li>
<li><p><strong>Automation</strong>: The pipeline was scheduled to run at 5-minute intervals, ensuring near-real-time data availability. I also implemented <strong>retry mechanisms</strong> and <strong>dead-letter queues</strong> to handle transient failures and ensure data integrity.</p>
</li>
</ul>
<h4 id="-2-data-processing-transforming-raw-logs-with-azure-databricks-and-pyspark-"><strong>2. Data Processing: Transforming Raw Logs with Azure Databricks and PySpark</strong></h4>
<p>Once the logs were ingested, the next challenge was to process and transform them into a structured format suitable for analysis. This is where <strong>Azure Databricks</strong> and <strong>PySpark</strong> came into play.</p>
<ul>
<li><p><strong>Design Pattern</strong>: I used a <strong>lambda architecture</strong>, combining batch and stream processing to handle both historical and real-time data. Azure Databricks served as the processing engine, while PySpark provided the computational power.</p>
</li>
<li><p><strong>Technical Implementation</strong>: I wrote PySpark scripts to clean, normalize, and enrich the raw logs. For example, for JSON and XML logs, I used PySparks&#39;s <code>inferSchema</code> functionality to automatically detect the structure of the logs. I also wrote custom parsers in Python to extract relevant fields. I also used <strong>statistical methods</strong> like <strong>Z-score analysis</strong> to identify anomalies in log patterns and filter out invalid logs: For instance, I calculated the Z-score for error rates, to filter out invalid logs:</p>
<pre><code>from pyspark.sql.functions import stddev, mean
df = df.filter(col(&quot;status_code&quot;).isin(200, 404, 500))
error_rates = df.groupBy(&quot;customer_id&quot;).agg(mean(&quot;error_count&quot;).alias(&quot;mean_error&quot;), stddev(&quot;error_count&quot;).alias(&quot;stddev_error&quot;))
error_rates = error_rates.withColumn(&quot;z_score&quot;, (col(&quot;error_count&quot;) - col(&quot;mean_error&quot;)) / col(&quot;stddev_error&quot;))
</code></pre>
<p>Logs with a Z-score greater than 3 were flagged as anomalies and routed for further investigation. If a log ingestion job failed (e.g., due to network issues or device unavailability), the system would automatically retry the job after a configurable delay. Logs that could not be processed after multiple retries were sent to a <strong>dead-letter queue</strong> for manual inspection and reprocessing, for example:

<pre><code>if retry_count &gt; MAX_RETRIES:
send_to_dead_letter_queue(<span class="hljs-name">log</span>)
</code></pre>
</li>
<li><p><strong>Scalability</strong>: Azure Databricks’ <strong>auto-scaling feature</strong>, in theory, allowed the system to handle varying data volumes efficiently.</p></li>
</ul>
<h4 id="-3-storage-optimizing-costs-and-performance-with-medallion-architecture-"><strong>3. Storage: Optimizing Costs and Performance with Medallion Architecture</strong></h4>


<p>To store the processed logs, I designed a <strong>medallion architecture</strong>, a multi-layered data storage pattern that balances cost, performance, and accessibility.</p>
<ul>
<li><p><strong>Bronze Layer</strong>: This layer stored raw, unprocessed logs in their original format. It served as a &quot;source of truth&quot; for debugging and auditing.</p>
</li>
<li><p><strong>Silver Layer</strong>: Here, logs were cleaned, normalized, and enriched. This layer was optimized for query performance, with indexed columns and partitioned data.</p>
</li>
<li><p><strong>Gold Layer</strong>: This layer contained aggregated and summarized data, such as daily error rates and security compliance metrics. It was designed for fast, analytical queries.</p>
</li>
<li><p><strong>Technical Implementation</strong>: I used <strong>Azure Data Lake Storage (ADLS)</strong> for the bronze layer, <strong>Delta Lake</strong> for the silver layer, and <strong>Azure SQL Database</strong> for the gold layer. Delta Lake’s <strong>ACID transactions</strong> and <strong>schema enforcement</strong> ensured data consistency and reliability.</p>
</li>
</ul>
<h4 id="-4-real-time-monitoring-and-alerts-power-bi-and-azure-logic-apps-"><strong>4. Real-Time Monitoring and Alerts: Power BI and Azure Logic Apps</strong></h4>
<p>To provide customers with real-time insights, I built a <strong>monitoring dashboard</strong> using <strong>Power BI</strong>. The dashboard visualized key metrics like server uptime, error rates, and security compliance status.</p>
<ul>
<li><p><strong>Automation</strong>: I configured <strong>Azure Logic Apps</strong> to send automated alerts for critical events, such as server downtime or security breaches. For example:
</p>
<pre><code>{
    <span class="hljs-string">"trigger"</span>: <span class="hljs-string">"When a new log is added"</span>,
    <span class="hljs-string">"conditions"</span>: [
        {
            <span class="hljs-string">"field"</span>: <span class="hljs-string">"status_code"</span>,
            <span class="hljs-string">"operator"</span>: <span class="hljs-string">"equals"</span>,
            <span class="hljs-string">"value"</span>: <span class="hljs-string">"500"</span>
        }
    ],
    <span class="hljs-string">"actions"</span>: [
        {
            <span class="hljs-string">"type"</span>: <span class="hljs-string">"SendEmail"</span>,
            <span class="hljs-string">"to"</span>: <span class="hljs-string">"admin@customer.com"</span>,
            <span class="hljs-string">"subject"</span>: <span class="hljs-string">"Critical Error Detected"</span>
        }
    ]
}
</code></pre>
</li>
</ul>
<hr>
<h3 id="-challenges-and-root-cause-analysis-"><strong>Challenges and Root Cause Analysis</strong></h3>
<p>One of the most challenging aspects of the project was diagnosing and resolving performance bottlenecks in the data processing pipeline. For example, during load testing, we noticed that certain PySpark jobs were taking significantly longer to complete.</p>
<ul>
<li><p><strong>Bug fix</strong>: Using <strong>Spark UI</strong>, I identified that the bottleneck was caused by <strong>data skew</strong>—a few partitions were processing significantly more data than others. This was due to uneven distribution of log timestamps.</p>
</li>
<p><strong>Solution</strong>: I implemented <strong>salting</strong>, a technique that distributes data more evenly across partitions. For example:
</p>
<pre><code>df = df.withColumn(&quot;salt&quot;, (col(&quot;timestamp&quot;) % 10))
df = df.repartition(&quot;salt&quot;)
</code></pre>
<p>  This change plus other necessary code reduced processing times by 40%.</p>
</ul>
<p>The <strong>medallion architecture</strong> (bronze, silver, gold layers) was implemented to optimize storage costs and query performance. However, this approach introduced several challenges:</p>
<ul>
<li><p><strong>Bug fix</strong>: Ensuring consistency across the bronze (raw), silver (cleaned), and gold (aggregated) layers required careful handling of schema changes and data updates.</p>
<p><strong>Solution</strong>: I used <strong>Delta Lake</strong> to enforce schema validation and enable ACID transactions.
</li>
</ul>
</li>
<ul><li><p><strong>Bug fix</strong>: Handling partial logs, a bug emerged where partial logs were ingested during network interruptions, leading to data corruption.</p>
</li>
<p><strong>Solution</strong>: I implemented a <strong>checksum validation</strong> step to detect and discard partial logs:</p>
<pre><code>def validate_log(log):
expected_checksum = calculate_checksum(<span class="hljs-built_in">log</span>)
actual_checksum = <span class="hljs-built_in">log</span>[<span class="hljs-string">"checksum"</span>]
<span class="hljs-built_in">return</span>expected_checksum == actual_checksum
</code></pre>
</ul>
<hr>
<h3 id="-impact-and-takeaways-"><strong>Impact and Takeaways</strong></h3>
<p>The centralized log management system delivered transformative results:</p>
<ul>
<li><p><strong>95% Reduction in Data Retrieval Time</strong>: Customers could access and analyze logs in near-real time, improving incident response times.</p>
</li>
<li><p><strong>60% Reduction in Storage Costs</strong>: The medallion architecture and Delta Lake optimizations significantly reduced storage costs.</p>
</li>
<li><p><strong>Enhanced Security and Compliance</strong>: Real-time monitoring and automated alerts helped customers maintain compliance and respond to security threats proactively.</p>
</li>
</ul>
<p>This project was a testament to the power of <strong>data-driven decision-making</strong> and <strong>continuous optimization</strong>. It also reinforced the importance of <strong>root cause analysis</strong> in building reliable, high-performance systems.</p>

</section>
<footer>
    <p>&copy; 2025 Taha Aziz</p>
</footer>
</div>
</body>